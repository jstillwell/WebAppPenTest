using HtmlAgilityPack;
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Net;
using System.Text;
using System.Threading.Tasks;
using System.Xml;
using System.Xml.Linq;
using WebAppPenTest.Common;

namespace WebAppPenTest.Spider {
    /// <summary>
    /// Point me at a url and I will walk the site.
    /// This implementation will eventually respect the robots.txt file
    /// </summary>
    public class Crawler {
        public string Robots;
        private bool RobotsTxtExists = false;
        Queue<WebPage> Pages;
        private string RootUrl { get; set; }
        private static List<string> JavascriptLibraries = new List<string>() {
            "jquery",
            "jquery.mobile",
            "jquery-ui",
            "bootstrap",
            "angular",
            "react",
            "modernizr",
            "mithril",
            "aurelia",
            "ember",
            "backbone",
            "meteor"
        };
        private List<string> _libraries = new List<string>();
        Domain.DapperWrapper Dapper;

        public Crawler(string rootUrl) {
            RootUrl = rootUrl;
            Pages = new Queue<WebPage>();
            Dapper = new Domain.DapperWrapper();
        }
        /// <summary>
        /// scan a url and look for links to other pages to crawl and identify useful elements on this page.
        /// </summary>
        /// <param name="baseUrl"></param>
        public Queue<WebPage> Crawl() {
            //add functionality to allow user to toggle respecting the robots.txt file
            //if (!RootUrl.Contains(":443") || !RootUrl.Contains("https") || !FindRobots()) {

            var client = new Request();
            Console.WriteLine(string.Format("Getting root URL: {0}", RootUrl));
            var response = client.HttpGet(RootUrl);

            var parsed = new HtmlDocument();
            parsed.LoadHtml(response);

            var page = ExtractNodes(parsed);

            //Dapper.Open();
            //Dapper.InsertResponse(page.ToString());
            var responseObj = new Common.Models.Packet {
                Destination = RootUrl,
                IsIncoming = false,
                Source = Environment.MachineName,
                Timestamp = DateTime.Now,
                Url = RootUrl
            };
            var domResponse = new Domain.Packet();
            domResponse.Insert(responseObj);

            //find all links and continue crawl minus menu.
            foreach (var link in page.Links) {
                try {
                    CrawlLink(string.Format("{0}{1}", string.Empty, link.Attributes["href"].Value));
                } catch (WebException) {

                } catch (Exception) {

                }
                //}
            }

            return Pages;
        }
        /// <summary>
        /// crawls the rest of the site.
        /// adds to the queue just like the root.
        /// </summary>
        /// <param name="url"></param>
        /// <returns></returns>
        public void CrawlLink(string url) {
            var client = new Request();
            if (!url.StartsWith("http") && !url.Contains(RootUrl)) {
                url = string.Format("{0}{1}", RootUrl, url);
            }
            Console.WriteLine(string.Format("Getting: {0}", url));
            var response = client.HttpGet(url);

            var parsed = new HtmlDocument();
            parsed.LoadHtml(response);

            ExtractNodes(parsed);
        }
        /// <summary>
        /// Handles the parsing of the HTML page that is passed in and adds it to the list of Pages
        /// </summary>
        /// <param name="parsed"></param>
        private WebPage ExtractNodes(HtmlDocument parsed) {
            var page = new WebPage();

            page.Document = parsed;
            page.DocType = parsed.DocumentNode.Descendants("!DOCTYPE");
            page.Inputs = parsed.DocumentNode.Descendants("input");
            page.Buttons = page.Inputs.Where(x => x.Name == "button");
            page.BootstrapButtons = parsed.DocumentNode.Descendants().Where(x => x.Attributes["class"].Value.Contains("btn"));
            page.SubmitButton = page.Inputs.Where(x => x.Attributes["tpye"].Value == "submit");
            page.Links = parsed.DocumentNode.Descendants("a");
            page.InternalLinks = page.Links.Where(x => x.Attributes["href"].Value != "http");
            page.ExternalLinks = page.Links.Where(x => x.Attributes["href"].Value.StartsWith("http"));
            page.Scripts = parsed.DocumentNode.Descendants("script");
            page.Css = parsed.DocumentNode.Descendants("link").Where(x => x.Attributes["rel"].Value == "stylesheet");
            page.Meta = parsed.DocumentNode.Descendants("meta");
            page.Charset = parsed.DeclaredEncoding;
            page.Viewport = page.Meta.Where(x => x.Attributes["name"].Value == "viewport");
            page.Images = parsed.DocumentNode.Descendants("img");
            page.Video = parsed.DocumentNode.Descendants("video");
            page.Audio = parsed.DocumentNode.Descendants("audio");
            page.Canvas = parsed.DocumentNode.Descendants("canvas");
            page.Libraries = InspectScripts(page.Scripts);
            page.MenuLinks = page.Links.Where(x => x.Attributes["role"].Value.ToLower() == "menu");
            page.NavClass = parsed.DocumentNode.Descendants("div").Where(x => x.Attributes["class"].Value.Contains("nav"));
            page.Nav = parsed.DocumentNode.Descendants("nav");

            Pages.Enqueue(page);

            return page;
        }
        /// <summary>
        /// Write the queue to a log.
        /// </summary>
        private void Log() {
            var page = Pages.Peek();
            using (var sw = new StreamWriter(@"C:\TEMP\crawler.log", true)) {
                sw.WriteLine("********** Start of a new Crawl **********");
                sw.WriteLine(string.Format("{0}", page));
                sw.WriteLine("");
                sw.WriteLine("");
            }
        }
        /// <summary>
        /// looks for a list of common scripts.
        /// list of scripts can be found as this enum
        /// WebAppPenTest.Spider.JavaScript.LibraryType
        /// </summary>
        /// <param name="scripts"></param>
        private List<string> InspectScripts(IEnumerable<HtmlAgilityPack.HtmlNode> scripts) {
            foreach (var script in scripts) {
                if (script.Attributes["src"] != null) {
                    var currentScript = script.Attributes["src"];

                    foreach (var commonScript in JavascriptLibraries) {
                        if (currentScript.Value == commonScript) {
                            _libraries.Add(currentScript.Value);
                        }
                    }
                }
            }

            return _libraries;
        }
        /// <summary>
        /// checks the given WebPage for signs that it is a login form.
        /// </summary>
        /// <param name="page"></param>
        /// <returns></returns>
        private bool HasLogin(WebPage page) {
            foreach (var input in page.Inputs) {
                var id = input.Attributes["id"].Value.ToLower();
                var type = input.Attributes["type"].Value;
                if (id.Contains("user") || id.Contains("login") || id.Contains("email")) {
                    return true;
                }
            }
            return false;
        }
        /// <summary>
        /// looks for a robots.txt and if it exists we parse it and adhere the the rules defined within.
        /// see: http://www.robotstxt.org/robotstxt.html for more
        /// </summary>
        /// <returns></returns>
        private bool FindRobots() {
            var client = new Request();
            try {
                var response = client.HttpGetStream(string.Format("{0}/robots.txt", RootUrl));
                //TODO: each line is a comment (preceded by #) or a key value pair seperated by a colon
                using (var sr = new StreamReader(response)) {
                    var read = sr.ReadToEnd();
                    //ex. this means dont crawl me
                    //User-agent: *
                    //Disallow: /

                    if (read.Contains("User-agent: *") && read.Contains("Disallow: /")) {
                        Console.WriteLine("Robots.txt file found. User agent disallowed.");
                        return true;
                    } else {
                        Robots = read;
                        return false;
                    }
                }
            } catch (Exception) {
                Console.WriteLine("No robots.txt file found.");
                return false;
            }
        }
    }
}